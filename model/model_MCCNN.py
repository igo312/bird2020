import keras
import tensorflow as tf
from keras.optimizers import Adam, SGD
from keras.layers import Conv2D, Dense, concatenate, DepthwiseConv2D, Input
from keras.layers import ReLU, BatchNormalization, Dropout, Flatten, MaxPooling2D
from keras.models import clone_model, Sequential, Model
import keras.backend as K
from keras_gradient_accumulation import GradientAccumulation

import os

class MCCNN():
    '''
    multi-channel-convolutional-nerual-network MCCNN\n
    Copyright 2020 Shengyong Ren. All Rights Reserved
--------------------------------------------------------------------------------------------------------------------
    The image datasets I generated is about three channel image datasets And the model is about processing these multi-
    channel images. Cause every channel is generated by ICA, so what I suppose is that every channel has its meanings.
    So the shallow layer I will use will extract features respectively and the filters will be as attention filter to
    active or inactive som channel.Then use normal filter to merge the features
--------------------------------------------------------------------------------------------------------------------
    '''

    def __init__(self, lr, class_num, Img_size, accumulation_steps):
        # Input shape
        self.img_rows, self.img_cols = Img_size[0], Img_size[1]
        self.channels = 1
        self.img_shape = (self.img_rows, self.img_cols, self.channels)

        self.src_flag = False
        self.disc_flag = False

        # src:source, tgt:target
        self.src_optimizer = Adam(lr)
        if accumulation_steps > 1 :
            self.src_optimizer = GradientAccumulation(self.src_optimizer, accumulation_steps=accumulation_steps,)

        # multi layer classify num dict
        self.clf_num = class_num

    def BN_CONV(self, x, FNum, FSize, strides=(1, 1)):
        # The activation is relu default(it will add other activation in future)
        x = Conv2D(FNum, FSize, padding='same', strides=strides)(x)
        x = BatchNormalization()(x)
        x = ReLU()(x)
        return x

    def BN_DepthConv(self, x, FSize, strides=(1, 1), depth_multiplier=1, use_bias=False):
        x = DepthwiseConv2D(FSize, padding='same', strides=strides, depth_multiplier=depth_multiplier, use_bias=use_bias)(x)
        x = BatchNormalization()(x)
        x = ReLU()(x)
        return x

    def BN_FC(self, x, FNum):
        x = Dense(FNum)(x)
        x = BatchNormalization()(x)
        x = ReLU()(x)
        return x


    def define_source_encoder(self, weights=None):
        # TODO（3/17） use average pooling.去掉全连接层也要试试
        self.source_encoder = Sequential()
        inp = Input(shape=self.img_shape)
        #  inp = Input(shape=[256, 256, 3])
        # 让初始层学习更多的粗略特征，用于保留
        # x = BatchNormalization()(inp)
        x1 = self.BN_DepthConv(inp[:,:,:,1],3)
        x2 = self.BN_DepthConv(inp[:,:,:,1],3)
        x3 = self.BN_DepthConv(inp[:,:,:,1],3)
        x1 = self.BN_CONV(x1[:, :, :, 1], 16, 3, strides=(2,2))
        x2 = self.BN_CONV(x2[:, :, :, 1], 16, 3, strides=(2,2))
        x3 = self.BN_CONV(x3[:, :, :, 2], 16, 3, strides=(2,2))
        x = concatenate([x1,x2,x3])
        x = self.BN_CONV(x, 16, 3, strides=(2,2))
        x = self.BN_CONV(x, 64, 3)
        x = self.res_block(x, 128, 3, strides=(2, 2))
        x = self.res_block(x, 256, 3, strides=(2, 2))

        # x = self.res_block(x, 361, 3)
        # x = MaxPooling2D()(x)
        self.source_encoder = Model(inputs=(inp), outputs=(x))
        self.src_flag = True

        if weights is not None:
            # 从HDF5文件中加载权重到当前模型中, 默认情况下模型的结构将保持不变。\
            # 如果想将权重载入不同的模型（有些层相同）中，则设置by_name=True，只有名字匹配的层才会载入权重
            self.source_encoder.load_weights(weights, by_name=True)
            # self.source_encoder.load_weights(weights)

    def get_source_classifier(self, model, weights=None, mode=None):
        # model已经经过编译了，只是采用了相应的input和output
        # x = GlobalAvgPool2D()(model.output)
        # Use globalAvgPool2D will cut off 3000000 (The total is 7200000)
        x = Flatten()(model.output)
        x = self.BN_FC(x, 128)
        x = Dropout(0.5)(x)
        print('The classify num is {}'.format(self.clf_num))
        x = Dense(self.clf_num, activation='softmax')(x)
        source_classifier_model = Model(inputs=(model.input), outputs=(x), name=self.clf_mode + '_clf_model')

        if weights is not None:
            print('Loading pre-trained classifier model')
            source_classifier_model.load_weights(weights)
        return source_classifier_model

    def define_target_encoder(self, weights=None):
        # src_flag：应该是用来判断是否生成模型，如果false则生成模型
        if not self.src_flag:
            self.define_source_encoder()
        # TODO:去看看clone_model这个函数怎么用
        # Clone any `Model` instance.
        with tf.device('/cpu:0'):
            self.target_encoder = clone_model(self.source_encoder)

        if weights is not None:
            self.target_encoder.load_weights(weights, by_name=True)

    def tensorboard_log(self, callback, names, logs, batch_no):

        for name, value in zip(names, logs):
            summary = tf.Summary()
            summary_value = summary.value.add()
            summary_value.simple_value = value
            summary_value.tag = name
            callback.writer.add_summary(summary, batch_no)
            callback.writer.flush()

    def train_source_model(self, source_gen, val_gen, model, epochs, steps_per_epoch, validation_steps, model_name, save_path,
                           save_interval=10, start_epoch=0, lr=0.01, C_state=True):
        # TODO: 加入loss融合训练， 增大batch的容量。
        optimizer = Adam(lr=lr)
        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        if not os.path.isdir(save_path):
            os.mkdir(save_path)

        # ModelCheckpoint 回调类允许你定义检查模型权重的位置，文件应如何命名，以及在什么情况下创建模型的 Checkpoint。
        saver = keras.callbacks.ModelCheckpoint(os.path.join(save_path, model_name + '{epoch:02d}-{val_acc:.2f}.hdf5'),
                                                monitor='val_loss',
                                                verbose=1,
                                                save_best_only=False,
                                                save_weights_only=True,
                                                mode='auto',
                                                period=save_interval)

        # 用于自动调节学习率，factor代表衰减因子
        scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=20, verbose=0,
                                                      mode='min')

        # 用于提前终止训练防止过拟合
        early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

        tensorboard_save = os.path.join(save_path, 'tensorboard' + '\\' + model_name)
        if not os.path.isdir(tensorboard_save):
            os.mkdir(tensorboard_save)

        visualizer = keras.callbacks.TensorBoard(log_dir=tensorboard_save,
                                                 histogram_freq=0,
                                                 write_graph=True,
                                                 write_images=False)
        # TODO:为什么一个fit_generator会创建多个tensorboard
        if C_state:
            callbacks = [saver, scheduler, visualizer]
        else:
            callbacks = []
        model.fit_generator(source_gen,
                            steps_per_epoch=steps_per_epoch,
                            epochs=epochs,
                            callbacks=callbacks,
                            validation_data=val_gen,
                            validation_steps=validation_steps,
                            initial_epoch=start_epoch,
                            )

    def eval_source_classifier(self, test_gen, model, dataset='target', domain='Source'):
        # TODO: target 和 source数据划分怎么分呢？
        model.compile(loss='categorical_crossentropy', optimizer=self.src_optimizer, metrics=['accuracy'])
        scores = model.evaluate_generator(test_gen, 100)
        print('Classifier Test loss:%.5f' % (scores[0]))
        print('Classifier Test accuracy:%.2f%%' % (float(scores[1]) * 100))
        # scores = model.evaluate_generator(test_gen, 100)
        # print('%s %s Classifier Test loss:%.5f' % (dataset.upper(), domain, scores[0]))
        # print('%s %s Classifier Test accuracy:%.2f%%' % (dataset.upper(), domain, float(scores[1]) * 100))


